{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9s7AXWgIrD0kHPhdtMgxZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Norawit29/emoji_model/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import RobertaTokenizerFast,RobertaConfig,RobertaForMaskedLM,DataCollatorForLanguageModeling,Trainer,TrainingArguments\n",
        "\n",
        "from datasets import load_from_disk, concatenate_datasets, load_dataset\n",
        "\n",
        "import accelerate\n",
        "import transformers\n"
      ],
      "metadata": {
        "id": "0u9-3hzKV-A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a tokenzer"
      ],
      "metadata": {
        "id": "pc9rVrmNB1A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Byte level BPE tokenizer\n",
        "\n",
        "# Define the path to the text files for tokenizer training\n",
        "paths = [\"path_to_text_files\"]\n",
        "\n",
        "# Initialize the Byte level BPE tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize the tokenizer training\n",
        "tokenizer.train(\n",
        "    files=paths,\n",
        "    vocab_size=52_000,\n",
        "    min_frequency=2,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\n",
        "        \"<s>\",\n",
        "        \"<pad>\",\n",
        "        \"</s>\",\n",
        "        \"<unk>\",\n",
        "        \"<mask>\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save the trained tokenizer model to a specified directory\n",
        "tokenizer.save_model(\"path_to_save_model_directory\")\n"
      ],
      "metadata": {
        "id": "7GvejPXhPnFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare dataset for model training"
      ],
      "metadata": {
        "id": "jos-QlG_CGM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The training dataset including\n",
        "20% of tweets including emoji ds (3,776,718),\n",
        "all of MIMIC III ds (2,083,180),\n",
        "all of MIMIC IV discharge summary ds (331,794),\n",
        "5% of pubmed ds (1,002,469)\n",
        "\n"
      ],
      "metadata": {
        "id": "EgjX9I-DQ8uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate multiple datasets into one\n",
        "dataset_tmp = concatenate_datasets([tweets_ds_n, mimic_ds_n, pubmed_ds_n, mimicIV_ds_n])\n",
        "\n",
        "# Split the concatenated dataset into training and validation sets\n",
        "dataset_split_tmp = dataset_tmp.train_test_split(test_size=0.2, seed=42, shuffle=True)\n",
        "\n",
        "trains_ds_tmp = dataset_split_tmp['train']\n",
        "vals_ds_tmp = dataset_split_tmp['test']\n",
        "\n",
        "# Define a function to tokenize and encode dataset examples\n",
        "def encode_dataset(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
        "\n",
        "# Tokenize and encode the training dataset\n",
        "trains_encode_tmp = trains_ds_tmp.map(encode_dataset, batched=True)\n",
        "trains_encode_tmp.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "# Tokenize and encode the validation dataset\n",
        "vals_encode_tmp = vals_ds_tmp.map(encode_dataset, batched=True)\n",
        "vals_encode_tmp.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "# Save the processed datasets to disk\n",
        "trains_encode_tmp.save_to_disk(\"path_to_save_training_dataset\")\n",
        "vals_encode_tmp.save_to_disk(\"path_to_save_validation_dataset\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LTDSLo80L-rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a transformer model"
      ],
      "metadata": {
        "id": "T7lWPS6nCnjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking available GPU\n",
        "\n",
        "import torch\n",
        "import torch.cuda\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print('*'*10)\n",
        "print(f'_CUDA version: ')\n",
        "!nvcc --version\n",
        "print('*'*10)\n",
        "print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
        "print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
        "print(f'Device Name: {torch.cuda.get_device_name()}')\n",
        "print('*'*10)\n",
        "print(f\"GPU IS AVAILABLE {torch.cuda.is_available()}\")\n"
      ],
      "metadata": {
        "id": "q7qXcQZ6Ax_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training and validation datasets\n",
        "train_dataset = load_from_disk(\"path_to_training_dataset\")\n",
        "val_dataset = load_from_disk(\"path_to_validation_dataset\")\n",
        "\n",
        "# Define the RoBERTa model configuration\n",
        "model_config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1\n",
        ")\n",
        "\n",
        "# Initialize the model on GPU if available\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = RobertaForMaskedLM(config=model_config).to(device)\n",
        "print('Number of parameters: ', model.num_parameters())\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"path_to_pretrained_tokenizer_model\")\n",
        "tokenizer.model_max_length = 512\n",
        "\n",
        "# Define the data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"path_to_output_directory\",\n",
        "    overwrite_output_dir=False,\n",
        "    evaluation_strategy='epoch',\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    seed=1,\n",
        "    auto_find_batch_size=True,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "# Create the trainer for our model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_model(\"path_to_save_trained_model\")\n"
      ],
      "metadata": {
        "id": "eKK38cEXLJAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a sentence transformer model\n",
        "(the code was from sentence transformer model official site\n",
        "https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/ms_marco/train_bi-encoder_mnrl.py)"
      ],
      "metadata": {
        "id": "EakgLU59D8Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
        "from datetime import datetime\n",
        "import gzip\n",
        "import os\n",
        "import tarfile\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import IterableDataset\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import pickle\n",
        "import argparse\n"
      ],
      "metadata": {
        "id": "dVZzTbrAD_li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read the MS Marco dataset\n",
        "data_folder = \" \"\n",
        "\n",
        "\n",
        "## Read the corpus files, that contain all the passages. Store them in the corpus dict\n",
        "\n",
        "corpus = {}         #dict in the format: passage_id -> passage. Stores all existent passages\n",
        "collection_filepath = os.path.join(data_folder, 'collection.tsv')\n",
        "if not os.path.exists(collection_filepath):\n",
        "    tar_filepath = os.path.join(data_folder, 'collection.tar.gz')\n",
        "    if not os.path.exists(tar_filepath):\n",
        "        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz', tar_filepath)\n",
        "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
        "        tar.extractall(path=data_folder)\n",
        "\n",
        "with open(collection_filepath, 'r', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        pid, passage = line.strip().split(\"\\t\")\n",
        "        pid = int(pid)\n",
        "        corpus[pid] = passage\n"
      ],
      "metadata": {
        "id": "OaPjymolExS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read the train queries, store in queries dict\n",
        "\n",
        "queries = {}        #dict in the format: query_id -> query. Stores all training queries\n",
        "queries_filepath = os.path.join(data_folder, 'queries.train.tsv')\n",
        "if not os.path.exists(queries_filepath):\n",
        "    tar_filepath = os.path.join(data_folder, 'queries.tar.gz')\n",
        "    if not os.path.exists(tar_filepath):\n",
        "        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz', tar_filepath)\n",
        "\n",
        "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
        "        tar.extractall(path=data_folder)\n",
        "\n",
        "\n",
        "with open(queries_filepath, 'r', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        qid, query = line.strip().split(\"\\t\")\n",
        "        qid = int(qid)\n",
        "        queries[qid] = query\n"
      ],
      "metadata": {
        "id": "9dmNMsqgF0oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a dict (qid, pid) -> ce_score that maps query-ids (qid) and paragraph-ids (pid)\n",
        "# to the CrossEncoder score computed by the cross-encoder/ms-marco-MiniLM-L-6-v2 model\n",
        "\n",
        "ce_scores_file = os.path.join(data_folder, 'cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz')\n",
        "if not os.path.exists(ce_scores_file):\n",
        "    util.http_get('https://huggingface.co/datasets/sentence-transformers/msmarco-hard-negatives/resolve/main/cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz', ce_scores_file)\n",
        "\n",
        "with gzip.open(ce_scores_file, 'rb') as fIn:\n",
        "    ce_scores = pickle.load(fIn)\n"
      ],
      "metadata": {
        "id": "IgoGffCpF4Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use hard-negatives that have been mined\n",
        "\n",
        "hard_negatives_filepath = os.path.join(data_folder, 'msmarco-hard-negatives.jsonl.gz')\n",
        "ce_score_margin = 3\n",
        "num_negs_per_system = 5\n",
        "\n",
        "train_queries = {}\n",
        "negs_to_use = None\n",
        "with gzip.open(hard_negatives_filepath, 'rt') as fIn:\n",
        "    for line in tqdm.tqdm(fIn):\n",
        "        data = json.loads(line)\n",
        "\n",
        "        #Get the positive passage ids\n",
        "        qid = data['qid']\n",
        "        pos_pids = data['pos']\n",
        "\n",
        "        if len(pos_pids) == 0:  #Skip entries without positives passages\n",
        "            continue\n",
        "\n",
        "        pos_min_ce_score = min([ce_scores[qid][pid] for pid in data['pos']])\n",
        "        ce_score_threshold = pos_min_ce_score - ce_score_margin\n",
        "\n",
        "        #Get the hard negatives\n",
        "        neg_pids = set()\n",
        "        if negs_to_use is None:\n",
        "            if args.negs_to_use is not None:    #Use specific system for negatives\n",
        "                negs_to_use = args.negs_to_use.split(\",\")\n",
        "            else:   #Use all systems\n",
        "                negs_to_use = list(data['neg'].keys())\n",
        "\n",
        "        for system_name in negs_to_use:\n",
        "            if system_name not in data['neg']:\n",
        "                continue\n",
        "\n",
        "            system_negs = data['neg'][system_name]\n",
        "            negs_added = 0\n",
        "            for pid in system_negs:\n",
        "                if ce_scores[qid][pid] > ce_score_threshold:\n",
        "                    continue\n",
        "\n",
        "                if pid not in neg_pids:\n",
        "                    neg_pids.add(pid)\n",
        "                    negs_added += 1\n",
        "                    if negs_added >= num_negs_per_system:\n",
        "                        break\n",
        "\n",
        "        if args.use_all_queries or (len(pos_pids) > 0 and len(neg_pids) > 0):\n",
        "            train_queries[data['qid']] = {'qid': data['qid'], 'query': queries[data['qid']], 'pos': pos_pids, 'neg': neg_pids}\n",
        "\n",
        "del ce_scores\n"
      ],
      "metadata": {
        "id": "Ec6vZz6uGOMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load previously train transformer model\n",
        "\n",
        "roberta = models.Transformer(\"path_to_save_trained_model\")\n",
        "pooler = models.Pooling(\n",
        "    roberta.get_word_embedding_dimension(),\n",
        "    pooling_mode_mean_tokens=True\n",
        ")\n",
        "\n",
        "model = SentenceTransformer(modules=[roberta, pooler])\n",
        "model.max_seq_length = 300\n",
        "\n"
      ],
      "metadata": {
        "id": "shpJ5ZrCGnRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create a custom MSMARCO dataset that returns triplets (query, positive, negative)\n",
        "# on-the-fly based on the information from the mined-hard-negatives jsonl file.\n",
        "\n",
        "class MSMARCODataset(Dataset):\n",
        "    def __init__(self, queries, corpus):\n",
        "        self.queries = queries\n",
        "        self.queries_ids = list(queries.keys())\n",
        "        self.corpus = corpus\n",
        "\n",
        "        for qid in self.queries:\n",
        "            self.queries[qid]['pos'] = list(self.queries[qid]['pos'])\n",
        "            self.queries[qid]['neg'] = list(self.queries[qid]['neg'])\n",
        "            random.shuffle(self.queries[qid]['neg'])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        query = self.queries[self.queries_ids[item]]\n",
        "        query_text = query['query']\n",
        "\n",
        "        pos_id = query['pos'].pop(0)    #Pop positive and add at end\n",
        "        pos_text = self.corpus[pos_id]\n",
        "        query['pos'].append(pos_id)\n",
        "\n",
        "        neg_id = query['neg'].pop(0)    #Pop negative and add at end\n",
        "        neg_text = self.corpus[neg_id]\n",
        "        query['neg'].append(neg_id)\n",
        "\n",
        "        return InputExample(texts=[query_text, pos_text, neg_text])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.queries)\n",
        ""
      ],
      "metadata": {
        "id": "X9LI0wA1GpmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For training the SentenceTransformer model, we need a dataset, a dataloader, and a loss used for training.\n",
        "\n",
        "train_dataset = MSMARCODataset(train_queries, corpus=corpus)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n"
      ],
      "metadata": {
        "id": "d3Ktm-z3GuIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          epochs=5,\n",
        "          warmup_steps=args.warmup_steps,\n",
        "          use_amp=True,\n",
        "          checkpoint_path=\"path_to_ckpt\",\n",
        "          checkpoint_save_steps=len(train_dataloader),\n",
        "          optimizer_params = {'lr': args.lr},\n",
        "          )\n",
        "\n",
        "# Save the model\n",
        "model.save(\"path_to_save_trained_sentence_transformer_model\")\n"
      ],
      "metadata": {
        "id": "FsRFlB1qG4N6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}